version: "3"

# minio base configs
# Settings and configurations that are common for all containers
x-minio-common: &minio-common
  image: quay.io/minio/minio:RELEASE.2025-01-20T14-49-07Z
  #command: server --console-address ":9001" http://minio{1...1}/data{1...2} # ":UI port" datastorage
  command: server --address=":19000" --console-address ":9001" http://minio{1...1}/data{1...2} # [:API port] [:UI port] [datastorage]
  #ports:
  expose:
    #- "9000" #S3 API operations (each server use to talk to each other)
    - "19000" #S3 API operations (each server use to talk to each other)
    - "9001" #browser access
  environment:
    MINIO_ROOT_USER: minioadmin
    MINIO_ROOT_PASSWORD: minioadmin
  healthcheck:
    test: ["CMD", "mc", "ready", "local"]
    interval: 5s
    timeout: 5s
    retries: 5
  networks:
      - bigdata

services:
  ##HDFS
  #namenode:
  #  image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
  #  container_name: namenode
  #  hostname: namenode
  #  restart: always
  #  ports:
  #    - 9870:9870 #UI
  #    - 9000:9000 #
  #  expose:
  #    - "9000"
  #  volumes:
  #    - hadoop_namenode:/hadoop/dfs/name
  #    - share_data:/partage
  #  environment:
  #    - CLUSTER_NAME=test
  #  env_file:
  #    - ./hadoop.env      
  #  #network_mode: host
  #  networks:
  #    - bigdata
  #    
  #datanode:
  #  image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
  #  container_name: datanode
  #  hostname: datanode
  #  restart: always
  #  ports:
  #    - "9864:9864"
  #    - "9866:9866"
  #    - "9867:9867"
  #    - "50010:50010"
  #    - "50020:50020"
  #  volumes:
  #    - hadoop_datanode:/hadoop/dfs/data
  #    - share_data:/partage
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:9870"
  #  env_file:
  #    - ./hadoop.env
  #  #network_mode: host
  #  networks:
  #    - bigdata
      
  #MINIO
  # url: https://github.com/minio/minio/tree/master/docs/orchestration/docker-compose
  # python acces:
  # https://min.io/docs/minio/linux/developers/python/API.html
  # https://min.io/docs/minio/linux/developers/python/minio-py.html
  minio1:
    <<: *minio-common
    hostname: minio1
    container_name: minio1
    restart: always
    volumes:
      - data1-1:/data1
      - data1-2:/data2
  
  ##minio2:
  ##  <<: *minio-common
  ##  hostname: minio2
  ##  container_name: minio2
  ##  restart: always
  ##  volumes:
  ##    - data2-1:/data1
  ##    - data2-2:/data2
  #
  nginx_minio:
    image: nginx:1.19.2-alpine
    hostname: nginx_minio
    container_name: nginx_minio
    restart: always
    volumes:
      - ./nginx_minio.conf:/etc/nginx/nginx.conf:ro
    ports:
      #- "9000:9000"
      - "19000:19000"
      - "9001:9001"
    depends_on:
      - minio1
  #    - minio2
  #    - minio3
  #    - minio4
    networks:
      - bigdata
  
  ##YARN      
  #resourcemanager:
  #  image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
  #  container_name: resourcemanager
  #  hostname: resourcemanager
  #  restart: always
  #  ports:
  #    - "8088:8088"
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
  #  env_file:
  #    - ./hadoop.env
  #  volumes:
  #    - share_data:/partage
  #  #network_mode: host
  #  networks:
  #    - bigdata
  #    
  #nodemanager:
  #  image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
  #  container_name: nodemanager
  #  hostname: nodemanager
  #  restart: always
  #  ports:
  #    - "8042:8042"
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088"
  #  env_file:
  #    - ./hadoop.env
  #  volumes:
  #    - share_data:/partage
  #  #network_mode: host
  #  networks:
  #    - bigdata
  #    
  #historyserver:
  #  image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
  #  container_name: historyserver
  #  hostname: historyserver
  #  restart: always
  #  ports:
  #    - "8188:8188"
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088"
  #  volumes:
  #    - hadoop_historyserver:/hadoop/yarn/timeline
  #    - share_data:/partage
  #  env_file:
  #    - ./hadoop.env      
  #  #network_mode: host      
  #  networks:
  #    - bigdata
  
  ## HIVE
  #hive-server:
  #  image: bde2020/hive:2.3.2-postgresql-metastore
  #  container_name: hive-server
  #  hostname: hive-server
  #  #network_mode: host
  #  networks:
  #    - bigdata    
  #  env_file:
  #    - ./hadoop-hive.env
  #    - ./hadoop.env
  #  environment:
  #    HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
  #    SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088 hive-metastore:9083"
  #  volumes:
  #    - share_data:/partage
  #  ports:
  #    - "10000:10000"
  #    - "10002:10002"
  #
  #hive-metastore:
  #  image: bde2020/hive:2.3.2-postgresql-metastore
  #  container_name: hive-metastore
  #  hostname: hive-metastore
  #  #network_mode: host
  #  networks:
  #    - bigdata
  #  env_file:
  #    - ./hadoop-hive.env
  #    - ./hadoop.env
  #  command: /opt/hive/bin/hive --service metastore
  #  environment:
  #    SERVICE_PRECONDITION: "namenode:9000 namenode:9870 resourcemanager:8088 hive-metastore-postgresql:5432"
  #  volumes:
  #    - share_data:/partage
  #  ports:
  #    - "9083:9083"
  #
  #hive-metastore-postgresql:
  #  image: bde2020/hive-metastore-postgresql:2.3.0
  #  container_name: hive-metastore-postgresql
  #  hostname: hive-metastore-postgresql
  #  #network_mode: host
  #  networks:
  #    - bigdata
  #  volumes:
  #    - share_data:/partage
  #  ports:
  #    - "5432:5432"

  # MYSQL
  nanp-mysql:
    image: mysql:8.0
    container_name: nanp-mysql
    hostname: nanp-mysql
    environment:
      - MYSQL_ROOT_PASSWORD=mysql
      - MYSQL_USER=nanp
      - MYSQL_PASSWORD=nanp
    volumes:
      - mysql_data:/var/lib/mysql
      - share_data:/partage
    #network_mode: host
    networks:
      - bigdata
    ports:
      - "8889:3306"  
  
  # MONGODB
  mongo:
    image: mongo:latest
    container_name: mongo
    hostname: mongo
    restart: always
    command:
      - '--logpath'
      - '/var/log/mongodb/mongod.log'
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: 12345678
    volumes:
      - mongo_data:/data/db
      - mongo_log:/var/log/mongodb
      - share_data:/partage
    networks:
      - bigdata
    ports:
      - 27017:27017
  
  ## UI MONGO (not needed, use compass|dbeaver|mongosh)
  #mongo-express:
  #  image: mongo-express
  #  container_name: mongo-express
  #  hostname: mongo-express
  #  restart: always
  #  ports:
  #    - 8081:8081
  #  environment:
  #    ME_CONFIG_MONGODB_ADMINUSERNAME: root
  #    ME_CONFIG_MONGODB_ADMINPASSWORD: 12345678
  #    ME_CONFIG_MONGODB_URL: mongodb://root:12345678@mongo:27017/
  #    ME_CONFIG_BASICAUTH: false
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
  


  # NIFI
  nanp-nifi:
    image: apache/nifi:1.24.0
    container_name: nanp-nifi
    hostname: nanp-nifi
    restart: always
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      #- NIFI_WEB_HTTP_PORT=8080
      - SINGLE_USER_CREDENTIALS_USERNAME=nifi_user
      - SINGLE_USER_CREDENTIALS_PASSWORD=lundimardi1234
    volumes:
      #- nifi_data:/opt/nifi/nifi-current/
      - share_data:/partage
    networks:
      - bigdata
    ports:
      - "8887:8080"
      - "8443:8443"
  
  ### KAFKA SIMPLE:
  ## url: https://github.com/conduktor/kafka-stack-docker-compose/blob/master/zk-single-kafka-single.yml
  ## config: https://docs.confluent.io/platform/current/installation/docker/config-reference.html#confluent-ak-configuration
  #zookeeper:
  #  image: confluentinc/cp-zookeeper:7.3.2
  #  hostname: zookeeper
  #  container_name: zookeeper
  #  restart: always
  #  ports:
  #    - "2181:2181"
  #  environment:
  #    ZOOKEEPER_CLIENT_PORT: 2181
  #    ZOOKEEPER_SERVER_ID: 1
  #    ZOOKEEPER_SERVERS: zookeeper:2888:3888
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
  #
  #kafka:
  #  image: confluentinc/cp-kafka:7.3.2
  #  hostname: kafka
  #  container_name: kafka
  #  restart: always
  #  ports:
  #    - "9092:9092"
  #    - "29092:29092"
  #    - "9999:9999"
  #  environment:
  #    KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
  #    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
  #    KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
  #    KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
  #    KAFKA_BROKER_ID: 1
  #    KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
  #    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #    KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
  #    KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
  #    KAFKA_JMX_PORT: 9999
  #    KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
  #    KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
  #    KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
  #  depends_on:
  #    - zookeeper
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
      
  ### KAFKA FULL (fast and full setup)
  ## video: https://www.youtube.com/watch?v=Lq-qoMKSQLo
  ## Docker hub: https://hub.docker.com/r/landoop/fast-data-dev
  ## github: https://github.com/soumilshah1995/DebeziumFlinkHudiSync/tree/main
  ## A Kafka distribution with Apache Kafka, Kafka Connect, Zookeeper, Confluent Schema Registry and REST Proxy
  ## Lenses.io Lenses or kafka-topics-ui, schema-registry-ui, kafka-connect-ui
  ## Lenses.io Stream Reactor, 25+ Kafka Connectors to simplify ETL processes
  ## Integration testing and examples embedded into the docker
  #kafka-full:
  #  #image: landoop/fast-data-dev:3.6
  #  image: dougdonohoe/fast-data-dev
  #  container_name: kafka-full
  #  hostname: kafka-full
  #  restart: always    
  #  environment:      
  #    - WEB_PORT=3040 #UI
  #    - REGISTRY_PORT=8081
  #    - ZK_PORT=3181
  #    - REST_PORT=7082
  #    - CONNECT_PORT=7083
  #    - BROKER_PORT=7092 #BROKER
  #    - ADV_HOST=127.0.0.1
  #    #- CONNECT_HEAP=2G
  #    - SAMPLEDATA=0
  #    - RUNNING_SAMPLEDATA=0
  #    #- RUNTESTS=0
  #    - CONNECTORS=debezium-mysql
  #    - DISABLE=azure-documentdb,blockchain,bloomberg,cassandra,coap,druid,elastic,elastic5,ftp,hazelcast,hbase,influxdb,jms,kudu,mongodb,mqtt,pulsar,redis,rethink,voltdb,couchbase,dbvisitreplicate,debezium-mongodb,debezium-postgres,elasticsearch,hdfs,jdbc,s3,twitter
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
  #  ports:
  #    - "3181:3181"
  #    - "3040:3040"
  #    - "7081:7081"
  #    - "7082:7082"
  #    - "7083:7083"
  #    - "7092:7092"
  #    - "8081:8081"
   
  ## SPARK
  #spark-master:
  #  image: bde2020/spark-master:3.3.0-hadoop3.3
  #  container_name: spark-master
  #  hostname: spark-master
  #  ports:
  #    - "8080:8080"
  #    - "7077:7077"
  #  environment:
  #    - INIT_DAEMON_STEP=setup_spark
  #    - HADOOP_CONF_DIR=/partage/xml_spark
  #    - SPARK_WORKER_CORES=1
  #    - SPARK_WORKER_MEMORY=1G
  #    - SPARK_DRIVER_MEMORY=1G
  #    - SPARK_EXECUTOR_MEMORY=1G
  #    - SPARK_MASTER=yarn
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
  #spark-worker:
  #  image: bde2020/spark-worker:3.3.0-hadoop3.3
  #  container_name: spark-worker
  #  hostname: spark-worker
  #  depends_on:
  #    - spark-master
  #  ports:
  #    - "8081:8081"
  #  environment:
  #    - "SPARK_MASTER=spark://spark-master:7077"
  #    - HADOOP_CONF_DIR=/partage/xml_spark
  #    - SPARK_WORKER_CORES=1
  #    - SPARK_WORKER_MEMORY=1G
  #    - SPARK_DRIVER_MEMORY=1G
  #    - SPARK_EXECUTOR_MEMORY=1G
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
  
  ## ELASTICSEARCH  
  #elasticsearch:
  #  #image: docker.elastic.co/elasticsearch/elasticsearch:7.4.0
  #  image: docker.elastic.co/elasticsearch/elasticsearch:8.3.2
  #  container_name: elasticsearch
  #  hostname: elasticsearch
  #  restart: always
  #  environment:
  #    - xpack.security.enabled=false
  #    - discovery.type=single-node
  #    - node.name=elasticsearch
  #    - cluster.name=docker-cluster
  #    ##- cluster.initial_master_nodes=elasticsearch
  #    - bootstrap.memory_lock=true
  #    - "ES_JAVA_OPTS=-Xms256M -Xmx256M"
  #    #- http.cors.enabled=true
  #    #- http.cors.allow-origin=*
  #    ##- network.host=_eth0_
  #  ulimits:
  #    memlock:
  #      soft: -1
  #      hard: -1
  #    nofile:
  #      soft: 65536
  #      hard: 65536
  #  cap_add:
  #    - IPC_LOCK
  #  volumes:
  #    - share_data:/partage
  #    #- ./elasticsearch-data:/usr/share/elasticsearch/data
  #    - elastic_data:/usr/share/elasticsearch/data
  #  ports:
  #    - 9200:9200
  #  networks:
  #    - bigdata
  
  ## KIBANA
  #kibana:
  #  container_name: kibana
  #  #image: docker.elastic.co/kibana/kibana:7.4.0
  #  image: docker.elastic.co/kibana/kibana:8.3.2
  #  hostname: kibana
  #  restart: always
  #  environment:
  #    - ELASTICSEARCH_HOSTS=http://elasticsearch:9200    # address of elasticsearch docker container which kibana will connect
  #  ports:
  #    - 5601:5601
  #  depends_on:
  #    - elasticsearch
  #  volumes:
  #    - share_data:/partage
  #  networks:
  #    - bigdata
    
volumes:
  #hadoop_namenode:
  #  driver: local # Define the driver and options under the volume name
  #  driver_opts:
  #    type: none
  #    device: C:\Users\ngoun\Downloads\music\docker_volume\volume_hadoop\namenode
  #    o: bind
  #hadoop_datanode:
  #  driver: local # Define the driver and options under the volume name
  #  driver_opts:
  #    type: none
  #    device: C:\Users\ngoun\Downloads\music\docker_volume\volume_hadoop\datanode
  #    o: bind
  data1-1:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: C:\Users\ngoun\Downloads\music\docker_volume\minio_volume\data1\data1_1
      o: bind
  data1-2:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: C:\Users\ngoun\Downloads\music\docker_volume\minio_volume\data1\data1_2
      o: bind
  #hadoop_historyserver:
  #  driver: local # Define the driver and options under the volume name
  #  driver_opts:
  #    type: none
  #    device: C:\Users\ngoun\Downloads\music\docker_volume\volume_hadoop\historyserver
  #    o: bind
  mysql_data:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: C:\Users\ngoun\Downloads\music\docker_volume\volume_mysql
      o: bind
  mongo_data:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: C:\Users\ngoun\Downloads\music\docker_volume\volume_mogodb\db
      o: bind
  mongo_log:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: C:\Users\ngoun\Downloads\music\docker_volume\volume_mogodb\log
      o: bind
  #elastic_data:
  #  driver: local # Define the driver and options under the volume name
  #  driver_opts:
  #    type: none
  #    device: C:\Users\ngoun\Downloads\music\docker_volume\volume_elastic
  #    o: bind
  #kibana_data:
  #  driver: local # Define the driver and options under the volume name
  #  driver_opts:
  #    type: none
  #    device: C:\Users\ngoun\Downloads\music\docker_volume\volume_kibana
  #    o: bind
  share_data:
    driver: local # Define the driver and options under the volume name
    driver_opts:
      type: none
      device: C:\Users\ngoun\Downloads\music\docker_volume\partage_nifi\partage
      o: bind
  #nifi_data:
  #  driver: local # Define the driver and options under the volume name
  #  driver_opts:
  #    type: none
  #    device: C:\Users\ngoun\Downloads\music\docker_volume\partage_nifi\nifi_data
  #    o: bind
  
networks:
  bigdata:
#    name: bigdata
#    external: true
